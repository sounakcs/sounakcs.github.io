<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Sounak Mondal</title>
  <link rel="icon" type="image/x-icon" href="resources/myicon.ico">
  <meta name="author" content="Sounak Mondal">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Sounak Mondal</name>
              </p>
              <p>I am a fourth year Computer Science PhD candidate at Stony Brook University interested in research on multimodal AI, specifically the intersection of Computer Vision and Natural Language Processing. My thesis is focused on exploring the interactions of vision and language influencing human attention. I am fortunate to be advised by <a href='https://www3.cs.stonybrook.edu/~minhhoai/'>Minh Hoai Nguyen</a> (dissertation advisor), <a href='https://www3.cs.stonybrook.edu/~samaras/'>Dimitris Samaras</a> and <a href='https://you.stonybrook.edu/zelinsky/'>Gregory Zelinsky</a>. I also collaborate with <a href='https://www3.cs.stonybrook.edu/~niranjan/'>Niranjan Balasubramanian</a>. </p>

              <p>Previously, I was an NLP Engineer at Samsung Research Institute, Bangalore, where I worked as part of the Natural Language Understanding team. Before that, I was an undergraduate student at the Department of Computer Science & Engineering, Jadavpur University, Kolkata, working on action detection and recognition in videos. </p>
              <p style="text-align:center">
                <a href="resources/cv.pdf">Résumé</a> &nbsp/&nbsp
                <a href="mailto:somondal@cs.stonybrook.edu">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=f9aUx9oAAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/sounak-mondal-283920198">LinkedIn</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="resources/profile.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="resources/profile.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

	      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:0px;width:100%;vertical-align:middle">
              <heading>News</heading>
              <ul>
  <li>[February 2024] One <a href="https://arxiv.org/abs/2303.09383">paper</a> accepted to CVPR 2024!
  <li>[November 2023] I will serve as a reviewer for CVPR 2024.</li>
  <li>[March 2023] One <a href="https://openaccess.thecvf.com/content/CVPR2023/html/Mondal_Gazeformer_Scalable_Effective_and_Fast_Prediction_of_Goal-Directed_Human_Attention_CVPR_2023_paper.html">paper</a> accepted to CVPR 2023!</li>
  <li>[March 2023] One <a href="https://arxiv.org/abs/2303.09383">preprint</a> is available on arXiv.</li>
  <li>[July 2022] One <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136640051.pdf">paper</a> accepted to ECCV 2022!</li>
</ul>
            </td>
          </tr>
        </tbody></table>
	      

	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:0px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I am broadly interested in Computer Vision, Natural Language Processing and Multimodal AI (Vision-Language Modeling). My PhD research focuses on modeling multimodal aspects of human attention as manifested through human eye gaze. For more details, refer to my <a href="resources/cv.pdf">résumé</a>.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

		<!--  HAT -->
              <tr>
                <td style="padding:2%;width:25%;vertical-align:middle">
                  <img src='resources/hat.jpg' width="100%" height="auto">
                </td>
                <td style="padding:2%;width:75%;vertical-align:middle">
                  <papertitle>Unifying Top-down and Bottom-up Scanpath Prediction using Transformers</papertitle>
                  <br>
                  <a href="https://www3.cs.stonybrook.edu/~zhibyang/">Zhibo Yang</a>,
                  <strong><span style="font-size: 15px">Sounak Mondal</span></strong>,
                  <a href="https://ahnchive.github.io/">Seoyoung Ahn</a>,
		  <a href="https://www.linkedin.com/in/ruoyu-xue">Ruoyu Xue</a>, 
                  <a href="https://you.stonybrook.edu/zelinsky/">Gregory Zelinsky</a>,
                  <a href="https://www3.cs.stonybrook.edu/~minhhoai/">Minh Hoai</a>,
                  <a href="https://www3.cs.stonybrook.edu/~samaras/">Dimitris Samaras</a>
                  <br>
                  <em>CVPR, 2024 </em> <br>
                  <a href="https://arxiv.org/abs/2303.09383">arXiv</a>
                  <p></p>
                </td>
              </tr> 
		
              <!--  Gazeformer -->
              <tr>
                <td style="padding:2%;width:25%;vertical-align:middle">
                  <img src='resources/gazeformer.jpg' width="100%" height="auto">
                </td>
                <td style="padding:2%;width:75%;vertical-align:middle">
                  <papertitle>Gazeformer: Scalable, Effective and Fast Prediction of Goal-Directed Human Attention</papertitle>
                  <br>
                  <strong><span style="font-size: 15px">Sounak Mondal</span></strong>
                  <a href="https://www3.cs.stonybrook.edu/~zhibyang/">Zhibo Yang</a>,
                  <a href="https://ahnchive.github.io/">Seoyoung Ahn</a>,
                  <a href="https://www3.cs.stonybrook.edu/~samaras/">Dimitris Samaras</a>,
                  <a href="https://you.stonybrook.edu/zelinsky/">Gregory Zelinsky</a>,
                  <a href="https://www3.cs.stonybrook.edu/~minhhoai/">Minh Hoai</a>
                  <br>
                  <em>CVPR, 2023 </em> <br>
                  <a href="https://arxiv.org/abs/2303.15274">arXiv</a>
                  /
                  <a href="https://openaccess.thecvf.com/content/CVPR2023/supplemental/Mondal_Gazeformer_Scalable_Effective_CVPR_2023_supplemental.pdf">Supplement</a>
                  /
                  <a href="https://github.com/cvlab-stonybrook/Gazeformer/">Code</a>
		  /
		  <a href="https://www.youtube.com/watch?v=5ACbxDmvLZU">Talk</a>
                  <p></p>
                </td>
              </tr> 

          

          <!--  FFM -->
              <tr>
                <td style="padding:2%;width:25%;vertical-align:middle">
                  <img src='resources/ffm.jpg' width="100%" height="auto">
                </td>
                <td style="padding:2%;width:75%;vertical-align:middle">
                  <papertitle>Target-absent Human Attention</papertitle>
                  <br>
                  <a href="https://www3.cs.stonybrook.edu/~zhibyang/">Zhibo Yang</a>,
                  <strong><span style="font-size: 15px">Sounak Mondal</span></strong>,
                  <a href="https://ahnchive.github.io/">Seoyoung Ahn</a>,
                  <a href="https://you.stonybrook.edu/zelinsky/">Gregory Zelinsky</a>,
                  <a href="https://www3.cs.stonybrook.edu/~minhhoai/">Minh Hoai</a>,
                  <a href="https://www3.cs.stonybrook.edu/~samaras/">Dimitris Samaras</a>
                  <br>
                  <em>ECCV, 2022 </em> <br>
                  <a href="https://arxiv.org/abs/2207.01166">arXiv</a>
                  /
                  <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136640051-supp.pdf">Supplement</a>
                  /
                  <a href="https://github.com/cvlab-stonybrook/Target-absent-Human-Attention">Code</a>
                  <p></p>
                </td>
              </tr> 

          <!--  CVPRW -->
              <tr>
                <td style="padding:2%;width:25%;vertical-align:middle">
                  <img src='resources/cvprw.jpg' width="100%" height="auto">
                </td>
                <td style="padding:2%;width:75%;vertical-align:middle">
                  <papertitle>Characterizing Target-absent Human Attention</papertitle>
                  <br>
                  <a href="https://sites.google.com/view/yupeichen/home/">Yupei Chen</a>,
                  <a href="https://www3.cs.stonybrook.edu/~zhibyang/">Zhibo Yang</a>,
                  <a href="https://www.linkedin.com/in/souradeep-chakraborty/">Souradeep Chakraborty</a>,
                  <strong><span style="font-size: 15px">Sounak Mondal</span></strong>,
                  <a href="https://ahnchive.github.io/">Seoyoung Ahn</a>,
                  <a href="https://www3.cs.stonybrook.edu/~samaras/">Dimitris Samaras</a>,
                  <a href="https://www3.cs.stonybrook.edu/~minhhoai/">Minh Hoai</a>,
                  <a href="https://you.stonybrook.edu/zelinsky/">Gregory Zelinsky</a>
                  <br>
                  <em>CVPR Workshop, 2022 </em> <br>
                  <a href="https://openaccess.thecvf.com/content/CVPR2022W/GAZE/papers/Chen_Characterizing_Target-Absent_Human_Attention_CVPRW_2022_paper.pdf">Paper</a>
                  /
                  <a href="https://openaccess.thecvf.com/content/CVPR2022W/GAZE/supplemental/Chen_Characterizing_Target-Absent_Human_CVPRW_2022_supplemental.pdf">Supplement</a>
                  <p></p>
                </td>
              </tr> 

          <!--  ICAN -->
              <tr>
                <td style="padding:2%;width:25%;vertical-align:middle">
                  <img src='resources/ican.jpg' width="100%" height="auto">
                </td>
                <td style="padding:2%;width:75%;vertical-align:middle">
                  <papertitle>ICAN: Introspective Convolutional Attention Network for Semantic Text Classification</papertitle>
                  <br>
                  <strong><span style="font-size: 15px">Sounak Mondal</span></strong>,
                  Suraj Modi*,
                  Sakshi Garg*,
                  Dhruva Das,
                  Siddhartha Mukherjee
                  <br>
                  <em>ICSC, 2020 (* indicates equal contribution)</em> <br>
                  <a href="https://ieeexplore.ieee.org/document/9031531">Paper</a>
                  <p></p>
                </td>
              </tr> 

          <!--  ICAPR -->
              <tr>
                <td style="padding:2%;width:25%;vertical-align:middle">
                  <img src='resources/icapr.jpg' width="100%" height="auto">
                </td>
                <td style="padding:2%;width:75%;vertical-align:middle">
                  <papertitle>Violent/Non-Violent Video Classification based on Deep Neural Network</papertitle>
                  <br>
                  <strong><span style="font-size: 15px">Sounak Mondal</span></strong>,
                  Soumyajit Pal,
                  <a href="https://jaduniv.irins.org/profile/57043">Sanjoy Kumar Saha</a>,
                  <a href="https://www.isical.ac.in/~chanda/">Bhabatosh Chanda</a>
                  <br>
                  <em>ICAPR, 2017</em> <br>
                  <a href="https://ieeexplore.ieee.org/document/8593000">Paper</a>
                  <p></p>
                </td>
              </tr> 

          <!--  ICVGIPW -->
              <tr>
                <td style="padding:2%;width:25%;vertical-align:middle">
                  <img src='resources/icvgip.jpg' width="100%" height="auto">
                </td>
                <td style="padding:2%;width:75%;vertical-align:middle">
                  <papertitle>A Beta Distribution Based Novel Scheme for Detection of Changes in Crowd Motion</papertitle>
                  <br>
                  Soumyajit Pal,
                  <strong><span style="font-size: 15px">Sounak Mondal</span></strong>,
                  <a href="https://jaduniv.irins.org/profile/57043">Sanjoy Kumar Saha</a>,
                  <a href="https://www.isical.ac.in/~chanda/">Bhabatosh Chanda</a>
                  <br>
                  <em>ICVGIP Workshop, 2016</em> <br>
                  <a href="https://link.springer.com/chapter/10.1007/978-3-319-68124-5_17">Paper</a>
                  <p></p>
                </td>
              </tr> 

           </tbody></table>
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tr>
              <td>
                <br>
                <p align="right">
                  <font size="1">
                    <a href="https://jonbarron.info/">Webpage template from Jon Barron</a>
                  </font>
                </p>
              </td>
            </tr>
    </tbody>
  </table>
				

      </td>
    </tr>
  </table>
</body>

</html>
