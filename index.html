<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Sounak Mondal</title>
  <link rel="icon" type="image/x-icon" href="resources/myicon.ico">
  <meta name="author" content="Sounak Mondal">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Sounak Mondal</name>
              </p>
              <p>I am a final year Computer Science PhD candidate at Stony Brook University interested in research on on multimodal learning, particularly vision-language modeling. My PhD thesis focuses on
using vision-language representation learning and multimodal foundation models (e.g., multimodal LLMs) for
modeling human visual attention (eye gaze). I am advised by <a href='https://www3.cs.stonybrook.edu/~minhhoai/'>Minh Hoai Nguyen</a> (dissertation advisor), <a href='https://www3.cs.stonybrook.edu/~samaras/'>Dimitris Samaras</a> and <a href='https://you.stonybrook.edu/zelinsky/'>Gregory Zelinsky</a>. I also collaborate with <a href='https://www3.cs.stonybrook.edu/~niranjan/'>Niranjan Balasubramanian</a>. </p>

              <p>Previously, I was an NLP Engineer at Samsung Research Institute, Bangalore, where I worked as part of the Natural Language Understanding team. Before that, I was an undergraduate student at the Department of Computer Science & Engineering, Jadavpur University, Kolkata, working on action detection and recognition in videos. </p>
              <p style="text-align:center">
                <a href="resources/cv.pdf">Résumé</a> &nbsp/&nbsp
                <a href="mailto:somondal@cs.stonybrook.edu">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=f9aUx9oAAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/sounak-mondal-283920198">LinkedIn</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="resources/profile.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="resources/profile.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

	      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:0px;width:100%;vertical-align:middle">
              <heading>News</heading>
              <ul>

  <li>[August 2025] <span style="color: red;"><strong>I am actively looking for full-time industry research scientist / applied scientist opportunities. Please contact me via email on LinkedIn if you have any leads. </strong></span></li>
  <li>[June 2025] One paper accepted to ICCV 2025!</li>
  <li>[June 2024] I have joined Meta Reality Labs Research (RLR), Burlingame as a Research Scientist Intern! </li>
  <li>[June 2025] I successfully defended my thesis proposal!</li>
  <li>[May 2025] I will serve as a reviewer for NeurIPS 2025.</li>
  <li>[March 2025] I will serve as a reviewer for ICCV 2025.</li>
  <li>[February 2025] One <a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Xue_Few-shot_Personalized_Scanpath_Prediction_CVPR_2025_paper.pdf">paper</a> accepted to CVPR 2025!
  <li>[November 2024] I will serve as a reviewer for CVPR 2025 and TPAMI.</li>
  <li>[October 2024] I will continue working at  Meta Reality Labs Research (RLR) remotely as a Part-Time Student Researcher. </li>
  <li>[July 2024] One <a href="https://arxiv.org/pdf/2407.19605">paper</a> on gaze prediction for object referral, and one <a href="https://arxiv.org/pdf/2406.02774">paper</a> on gaze following accepted to ECCV 2024!
  <li>[June 2024] I have joined Meta Reality Labs Research (RLR), Redmond as a Research Scientist Intern! </li>
  <li>[February 2024] One <a href="https://arxiv.org/abs/2303.09383">paper</a> accepted to CVPR 2024!
  <li>[November 2023] I will serve as a reviewer for CVPR 2024.</li>
  <li>[March 2023] One <a href="https://openaccess.thecvf.com/content/CVPR2023/html/Mondal_Gazeformer_Scalable_Effective_and_Fast_Prediction_of_Goal-Directed_Human_Attention_CVPR_2023_paper.html">paper</a> accepted to CVPR 2023!</li>
  <li>[March 2023] One <a href="https://arxiv.org/abs/2303.09383">preprint</a> is available on arXiv.</li>
  <li>[July 2022] One <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136640051.pdf">paper</a> accepted to ECCV 2022!</li>
</ul>
            </td>
          </tr>
        </tbody></table>
	      

	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:0px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I am broadly interested in Computer Vision, Natural Language Processing and Multimodal AI (Vision-Language Modeling). My PhD research focuses on using vision-language representation learning and multimodal foundation models (e.g., multimodal LLMs) for
modeling human visual attention (eye gaze). For more details, refer to my <a href="resources/cv.pdf">résumé</a>.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


		<!-- GLAD-GLAM -->
		 <tr>
                <td style="padding:2%;width:25%;vertical-align:middle">
                  <img src='resources/glad.png' width="100%" height="auto">
                </td>
                <td style="padding:2%;width:75%;vertical-align:middle">
                  <papertitle>Gaze-Language Alignment for Zero-Shot Prediction of Visual Search Targets from Human Gaze Scanpaths</papertitle>
                  <br>
                  <strong><span style="font-size: 15px">Sounak Mondal</span></strong>,
                  <a href="https://scholar.google.com/citations?user=xo88uiUAAAAJ&hl=en/">Naveen Sendhilnathan</a>,
		  <a href="https://scholar.google.com/citations?user=ISMh-tUAAAAJ&hl=en">Ting Zhang</a>,
		<a href="https://openreview.net/profile?id=~Yue_Liu34/">Yue Liu</a>,
                  <a href="https://scholar.google.com/citations?user=oU9uSvYAAAAJ">Michael Proulx</a>,
                  <a href="https://scholar.google.com/citations?user=cjmjU5AAAAAJ&hl=en">Michael Iuzzolino</a>,
                  <a href="https://openreview.net/profile?id=~Chuan_Qin6">Chuan Qin</a>,
					<a href="https://tanyajonker.com/">Tanya Jonker</a>
                  <br>
                  <em>ICCV, 2025 </em> <br>
					<p></p>
                </td>
              </tr> 


		<!--  FS-PSP -->
              <tr>
                <td style="padding:2%;width:25%;vertical-align:middle">
                  <img src='resources/fspsp.png' width="100%" height="auto">
                </td>
                <td style="padding:2%;width:75%;vertical-align:middle">
                  <papertitle>Few-shot Personalized Scanpath Prediction</papertitle>
                  <br>
                  <a href="https://www.linkedin.com/in/ruoyu-xue">Ruoyu Xue</a>,
                  <a href="https://jingyixu.net/">Jingyi Xu</a>,
		  <strong><span style="font-size: 15px">Sounak Mondal</span></strong>,
					<a href="https://hieulem.github.io/">Hieu Le</a>,
					<a href="https://you.stonybrook.edu/zelinsky/">Gregory Zelinsky</a>,
                  <a href="https://www3.cs.stonybrook.edu/~minhhoai/">Minh Hoai</a>,
                  <a href="https://www3.cs.stonybrook.edu/~samaras/">Dimitris Samaras</a>
                  <br>
                  <em>CVPR, 2025 </em> <br>
                  <a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Xue_Few-shot_Personalized_Scanpath_Prediction_CVPR_2025_paper.pdf">Paper</a>
                  <p></p>
                </td>
              </tr> 
			
		<!--  ART -->
              <tr>
                <td style="padding:2%;width:25%;vertical-align:middle">
                  <img src='resources/art.jpg' width="100%" height="auto">
                </td>
                <td style="padding:2%;width:75%;vertical-align:middle">
                  <papertitle>Look Hear: Gaze Prediction for Speech-directed Human Attention</papertitle>
                  <br>
                  <strong><span style="font-size: 15px">Sounak Mondal</span></strong>,
                  <a href="https://ahnchive.github.io/">Seoyoung Ahn</a>,
		  <a href="https://www3.cs.stonybrook.edu/~zhibyang/">Zhibo Yang</a>,
		<a href="https://www3.cs.stonybrook.edu/~niranjan/">Niranjan Balasubramanian</a>,
                  <a href="https://www3.cs.stonybrook.edu/~samaras/">Dimitris Samaras</a>,
                  <a href="https://you.stonybrook.edu/zelinsky/">Gregory Zelinsky</a>,
                  <a href="https://www3.cs.stonybrook.edu/~minhhoai/">Minh Hoai</a>
                  <br>
                  <em>ECCV, 2024 </em> <br>
			<a href="https://arxiv.org/pdf/2407.19605">arXiv</a>
			/
                  <a href="https://sites.google.com/view/refcoco-gaze">Project Page</a>
                  /
                  <a href="https://github.com/cvlab-stonybrook/ART/">Code</a>
		  /
		  <a href="https://github.com/cvlab-stonybrook/refcoco-gaze">Dataset</a>
		  /
		  <a href="https://www.youtube.com/watch?v=hiP4y98-mQ4">Talk</a>
                  <p></p>
                </td>
              </tr> 

		
		<!--  VQA-Diffusion -->
              <tr>
                <td style="padding:2%;width:25%;vertical-align:middle">
                  <img src='resources/vqa.jpg' width="100%" height="auto">
                </td>
                <td style="padding:2%;width:75%;vertical-align:middle">
                  <papertitle>Diffusion-Refined VQA Annotations for Semi-Supervised Gaze Following</papertitle>
                  <br>
                  <a href="https://www.linkedin.com/in/qiaomu-miao-104083163">Qiaomu Miao</a>,
                  <a href="https://alexgraikos.github.io/">Alexandros Graikos</a>,
		  <a href="https://scholar.google.com/citations?user=8yA5YncAAAAJ&hl=en">Jingwei Zhang</a>,
		  <strong><span style="font-size: 15px">Sounak Mondal</span></strong>,
                  <a href="https://www3.cs.stonybrook.edu/~minhhoai/">Minh Hoai</a>,
                  <a href="https://www3.cs.stonybrook.edu/~samaras/">Dimitris Samaras</a>
                  <br>
                  <em>ECCV, 2024 </em> <br>
                  <a href="https://arxiv.org/pdf/2406.02774">arXiv</a>
                  <p></p>
                </td>
              </tr> 


		
		<!--  HAT -->
              <tr>
                <td style="padding:2%;width:25%;vertical-align:middle">
                  <img src='resources/hat.jpg' width="100%" height="auto">
                </td>
                <td style="padding:2%;width:75%;vertical-align:middle">
                  <papertitle>Unifying Top-down and Bottom-up Scanpath Prediction using Transformers</papertitle>
                  <br>
                  <a href="https://www3.cs.stonybrook.edu/~zhibyang/">Zhibo Yang</a>,
                  <strong><span style="font-size: 15px">Sounak Mondal</span></strong>,
                  <a href="https://ahnchive.github.io/">Seoyoung Ahn</a>,
		  <a href="https://www.linkedin.com/in/ruoyu-xue">Ruoyu Xue</a>, 
                  <a href="https://you.stonybrook.edu/zelinsky/">Gregory Zelinsky</a>,
                  <a href="https://www3.cs.stonybrook.edu/~minhhoai/">Minh Hoai</a>,
                  <a href="https://www3.cs.stonybrook.edu/~samaras/">Dimitris Samaras</a>
                  <br>
                  <em>CVPR, 2024 </em> <br>
                  <a href="https://arxiv.org/abs/2303.09383">arXiv</a>
                  <p></p>
                </td>
              </tr> 


		
              <!--  Gazeformer -->
              <tr>
                <td style="padding:2%;width:25%;vertical-align:middle">
                  <img src='resources/gazeformer.jpg' width="100%" height="auto">
                </td>
                <td style="padding:2%;width:75%;vertical-align:middle">
                  <papertitle>Gazeformer: Scalable, Effective and Fast Prediction of Goal-Directed Human Attention</papertitle>
                  <br>
                  <strong><span style="font-size: 15px">Sounak Mondal</span></strong>,
                  <a href="https://www3.cs.stonybrook.edu/~zhibyang/">Zhibo Yang</a>,
                  <a href="https://ahnchive.github.io/">Seoyoung Ahn</a>,
                  <a href="https://www3.cs.stonybrook.edu/~samaras/">Dimitris Samaras</a>,
                  <a href="https://you.stonybrook.edu/zelinsky/">Gregory Zelinsky</a>,
                  <a href="https://www3.cs.stonybrook.edu/~minhhoai/">Minh Hoai</a>
                  <br>
                  <em>CVPR, 2023 </em> <br>
                  <a href="https://arxiv.org/abs/2303.15274">arXiv</a>
                  /
                  <a href="https://openaccess.thecvf.com/content/CVPR2023/supplemental/Mondal_Gazeformer_Scalable_Effective_CVPR_2023_supplemental.pdf">Supplement</a>
                  /
                  <a href="https://github.com/cvlab-stonybrook/Gazeformer/">Code</a>
		  /
		  <a href="https://www.youtube.com/watch?v=5ACbxDmvLZU">Talk</a>
                  <p></p>
                </td>
              </tr> 

          

          <!--  FFM -->
              <tr>
                <td style="padding:2%;width:25%;vertical-align:middle">
                  <img src='resources/ffm.jpg' width="100%" height="auto">
                </td>
                <td style="padding:2%;width:75%;vertical-align:middle">
                  <papertitle>Target-absent Human Attention</papertitle>
                  <br>
                  <a href="https://www3.cs.stonybrook.edu/~zhibyang/">Zhibo Yang</a>,
                  <strong><span style="font-size: 15px">Sounak Mondal</span></strong>,
                  <a href="https://ahnchive.github.io/">Seoyoung Ahn</a>,
                  <a href="https://you.stonybrook.edu/zelinsky/">Gregory Zelinsky</a>,
                  <a href="https://www3.cs.stonybrook.edu/~minhhoai/">Minh Hoai</a>,
                  <a href="https://www3.cs.stonybrook.edu/~samaras/">Dimitris Samaras</a>
                  <br>
                  <em>ECCV, 2022 </em> <br>
                  <a href="https://arxiv.org/abs/2207.01166">arXiv</a>
                  /
                  <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136640051-supp.pdf">Supplement</a>
                  /
                  <a href="https://github.com/cvlab-stonybrook/Target-absent-Human-Attention">Code</a>
                  <p></p>
                </td>
              </tr> 

          <!--  CVPRW -->
              <tr>
                <td style="padding:2%;width:25%;vertical-align:middle">
                  <img src='resources/cvprw.jpg' width="100%" height="auto">
                </td>
                <td style="padding:2%;width:75%;vertical-align:middle">
                  <papertitle>Characterizing Target-absent Human Attention</papertitle>
                  <br>
                  <a href="https://sites.google.com/view/yupeichen/home/">Yupei Chen</a>,
                  <a href="https://www3.cs.stonybrook.edu/~zhibyang/">Zhibo Yang</a>,
                  <a href="https://www.linkedin.com/in/souradeep-chakraborty/">Souradeep Chakraborty</a>,
                  <strong><span style="font-size: 15px">Sounak Mondal</span></strong>,
                  <a href="https://ahnchive.github.io/">Seoyoung Ahn</a>,
                  <a href="https://www3.cs.stonybrook.edu/~samaras/">Dimitris Samaras</a>,
                  <a href="https://www3.cs.stonybrook.edu/~minhhoai/">Minh Hoai</a>,
                  <a href="https://you.stonybrook.edu/zelinsky/">Gregory Zelinsky</a>
                  <br>
                  <em>CVPR Workshop, 2022 </em> <br>
                  <a href="https://openaccess.thecvf.com/content/CVPR2022W/GAZE/papers/Chen_Characterizing_Target-Absent_Human_Attention_CVPRW_2022_paper.pdf">Paper</a>
                  /
                  <a href="https://openaccess.thecvf.com/content/CVPR2022W/GAZE/supplemental/Chen_Characterizing_Target-Absent_Human_CVPRW_2022_supplemental.pdf">Supplement</a>
                  <p></p>
                </td>
              </tr> 

          <!--  ICAN -->
              <tr>
                <td style="padding:2%;width:25%;vertical-align:middle">
                  <img src='resources/ican.jpg' width="100%" height="auto">
                </td>
                <td style="padding:2%;width:75%;vertical-align:middle">
                  <papertitle>ICAN: Introspective Convolutional Attention Network for Semantic Text Classification</papertitle>
                  <br>
                  <strong><span style="font-size: 15px">Sounak Mondal</span></strong>,
                  Suraj Modi*,
                  Sakshi Garg*,
                  Dhruva Das,
                  Siddhartha Mukherjee
                  <br>
                  <em>ICSC, 2020 (* indicates equal contribution)</em> <br>
                  <a href="https://ieeexplore.ieee.org/document/9031531">Paper</a>
                  <p></p>
                </td>
              </tr> 

          <!--  ICAPR -->
              <tr>
                <td style="padding:2%;width:25%;vertical-align:middle">
                  <img src='resources/icapr.jpg' width="100%" height="auto">
                </td>
                <td style="padding:2%;width:75%;vertical-align:middle">
                  <papertitle>Violent/Non-Violent Video Classification based on Deep Neural Network</papertitle>
                  <br>
                  <strong><span style="font-size: 15px">Sounak Mondal</span></strong>,
                  Soumyajit Pal,
                  <a href="https://jaduniv.irins.org/profile/57043">Sanjoy Kumar Saha</a>,
                  <a href="https://www.isical.ac.in/~chanda/">Bhabatosh Chanda</a>
                  <br>
                  <em>ICAPR, 2017</em> <br>
                  <a href="https://ieeexplore.ieee.org/document/8593000">Paper</a>
                  <p></p>
                </td>
              </tr> 

          <!--  ICVGIPW -->
              <tr>
                <td style="padding:2%;width:25%;vertical-align:middle">
                  <img src='resources/icvgip.jpg' width="100%" height="auto">
                </td>
                <td style="padding:2%;width:75%;vertical-align:middle">
                  <papertitle>A Beta Distribution Based Novel Scheme for Detection of Changes in Crowd Motion</papertitle>
                  <br>
                  Soumyajit Pal,
                  <strong><span style="font-size: 15px">Sounak Mondal</span></strong>,
                  <a href="https://jaduniv.irins.org/profile/57043">Sanjoy Kumar Saha</a>,
                  <a href="https://www.isical.ac.in/~chanda/">Bhabatosh Chanda</a>
                  <br>
                  <em>ICVGIP Workshop, 2016</em> <br>
                  <a href="https://link.springer.com/chapter/10.1007/978-3-319-68124-5_17">Paper</a>
                  <p></p>
                </td>
              </tr> 

           </tbody></table>
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tr>
              <td>
                <br>
                <p align="right">
                  <font size="1">
                    <a href="https://jonbarron.info/">Webpage template from Jon Barron</a>
                  </font>
                </p>
              </td>
            </tr>
    </tbody>
  </table>
				

      </td>
    </tr>
  </table>
</body>

</html>
